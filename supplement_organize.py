#!/usr/bin/env python3
"""
è¡¥å……æ•´ç†è„šæœ¬ - å¤„ç†é—æ¼çš„é…ç½®æ–‡ä»¶å’Œæ•°æ®é›†æ•´ç†
"""
import os
import shutil
import sys
from pathlib import Path
from datetime import datetime

class SupplementOrganizer:
    def __init__(self, project_root=None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.configs_dir = self.project_root / "configs"
        self.data_dir = self.project_root / "data"
        self.log_file = self.project_root / "supplement_organize.log"
        
    def log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def get_missing_items(self):
        """è·å–é—æ¼çš„é…ç½®æ–‡ä»¶å’Œéœ€è¦æ•´ç†çš„æ•°æ®é›†"""
        return {
            # é—æ¼çš„é…ç½®æ–‡ä»¶
            'config_files': [
                'config.json',  # æ ¹ç›®å½•çš„ä¸»é…ç½®æ–‡ä»¶
                'dataset_classify/config.json',  # åˆ†ç±»æ•°æ®é›†é…ç½®
                'dataset_info.json'  # æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
            ],
            # éœ€è¦æ•´ç†çš„æ•°æ®é›†ç›®å½•
            'dataset_dirs': [
                'dataset',
                'dataset_classify'
            ],
            # å…¶ä»–å¯èƒ½é—æ¼çš„æ–‡ä»¶
            'other_files': [
                'dataset.zip'  # æ•°æ®é›†å‹ç¼©åŒ…
            ]
        }
    
    def preview_supplement(self):
        """é¢„è§ˆè¡¥å……æ•´ç†æ“ä½œ"""
        self.log("é¢„è§ˆè¡¥å……æ•´ç†æ“ä½œ...")
        
        missing_items = self.get_missing_items()
        
        print("\nğŸ“ å°†è¦ç§»åŠ¨åˆ° configs/ çš„é…ç½®æ–‡ä»¶:")
        print("=" * 60)
        existing_configs = []
        missing_configs = []
        
        for config_file in missing_items['config_files']:
            config_path = self.project_root / config_file
            if config_path.exists():
                existing_configs.append(config_file)
                target_name = config_path.name
                if config_file == 'dataset_classify/config.json':
                    target_name = 'dataset_classify_config.json'
                print(f"âœ… {config_file} â†’ configs/{target_name}")
            else:
                missing_configs.append(config_file)
                print(f"âŒ {config_file} (ä¸å­˜åœ¨)")
        
        print("\nğŸ’¾ æ•°æ®é›†ç›®å½•æ•´ç†æ–¹æ¡ˆ:")
        print("=" * 60)
        existing_datasets = []
        missing_datasets = []
        
        for dataset_dir in missing_items['dataset_dirs']:
            dataset_path = self.project_root / dataset_dir
            if dataset_path.exists():
                existing_datasets.append(dataset_dir)
                print(f"âœ… {dataset_dir}/ â†’ data/{dataset_dir}/ (ç§»åŠ¨)")
            else:
                missing_datasets.append(dataset_dir)
                print(f"âŒ {dataset_dir}/ (ä¸å­˜åœ¨)")
        
        print("\nğŸ“¦ å…¶ä»–æ–‡ä»¶:")
        print("=" * 60)
        existing_others = []
        missing_others = []
        
        for other_file in missing_items['other_files']:
            other_path = self.project_root / other_file
            if other_path.exists():
                existing_others.append(other_file)
                print(f"âœ… {other_file} â†’ data/{other_file}")
            else:
                missing_others.append(other_file)
                print(f"âŒ {other_file} (ä¸å­˜åœ¨)")
        
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  - å¯ç§»åŠ¨é…ç½®æ–‡ä»¶: {len(existing_configs)}")
        print(f"  - å¯ç§»åŠ¨æ•°æ®é›†ç›®å½•: {len(existing_datasets)}")
        print(f"  - å¯ç§»åŠ¨å…¶ä»–æ–‡ä»¶: {len(existing_others)}")
        print(f"  - ç¼ºå¤±é¡¹ç›®: {len(missing_configs) + len(missing_datasets) + len(missing_others)}")
        
        return existing_configs, existing_datasets, existing_others
    
    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        self.log("åˆ›å»ºç›®å½•ç»“æ„...")
        
        # ç¡®ä¿configsç›®å½•å­˜åœ¨
        self.configs_dir.mkdir(exist_ok=True)
        self.log(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {self.configs_dir}")
        
        # åˆ›å»ºdataç›®å½•
        self.data_dir.mkdir(exist_ok=True)
        self.log(f"âœ… åˆ›å»ºç›®å½•: {self.data_dir}")
        
        return True
    
    def move_config_files(self, existing_configs):
        """ç§»åŠ¨é…ç½®æ–‡ä»¶åˆ°configsç›®å½•"""
        self.log("ç§»åŠ¨é…ç½®æ–‡ä»¶...")
        
        success_count = 0
        for config_file in existing_configs:
            try:
                src_path = self.project_root / config_file
                
                # ç¡®å®šç›®æ ‡æ–‡ä»¶å
                if config_file == 'dataset_classify/config.json':
                    target_name = 'dataset_classify_config.json'
                else:
                    target_name = src_path.name
                
                dest_path = self.configs_dir / target_name
                
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if dest_path.exists():
                    dest_path.unlink()
                
                # å¤åˆ¶æ–‡ä»¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼Œå› ä¸ºå¯èƒ½è¢«å…¶ä»–è„šæœ¬ä½¿ç”¨ï¼‰
                shutil.copy2(str(src_path), str(dest_path))
                self.log(f"âœ… å¤åˆ¶é…ç½®æ–‡ä»¶: {config_file} â†’ configs/{target_name}")
                success_count += 1
            except Exception as e:
                self.log(f"âŒ å¤åˆ¶é…ç½®æ–‡ä»¶å¤±è´¥ {config_file}: {str(e)}")
        
        return success_count
    
    def organize_datasets(self, existing_datasets, existing_others):
        """æ•´ç†æ•°æ®é›†ç›®å½•"""
        self.log("æ•´ç†æ•°æ®é›†ç›®å½•...")
        
        success_count = 0
        
        # ç§»åŠ¨æ•°æ®é›†ç›®å½•
        for dataset_dir in existing_datasets:
            try:
                src_path = self.project_root / dataset_dir
                dest_path = self.data_dir / dataset_dir
                
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if dest_path.exists():
                    shutil.rmtree(dest_path)
                
                shutil.move(str(src_path), str(dest_path))
                self.log(f"âœ… ç§»åŠ¨æ•°æ®é›†: {dataset_dir}/ â†’ data/{dataset_dir}/")
                success_count += 1
            except Exception as e:
                self.log(f"âŒ ç§»åŠ¨æ•°æ®é›†å¤±è´¥ {dataset_dir}: {str(e)}")
        
        # ç§»åŠ¨å…¶ä»–æ–‡ä»¶
        for other_file in existing_others:
            try:
                src_path = self.project_root / other_file
                dest_path = self.data_dir / other_file
                
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if dest_path.exists():
                    dest_path.unlink()
                
                shutil.move(str(src_path), str(dest_path))
                self.log(f"âœ… ç§»åŠ¨æ–‡ä»¶: {other_file} â†’ data/{other_file}")
                success_count += 1
            except Exception as e:
                self.log(f"âŒ ç§»åŠ¨æ–‡ä»¶å¤±è´¥ {other_file}: {str(e)}")
        
        return success_count
    
    def create_data_symlinks(self):
        """åœ¨æ ¹ç›®å½•åˆ›å»ºæ•°æ®é›†çš„ç¬¦å·é“¾æ¥ä»¥ä¿æŒå…¼å®¹æ€§"""
        self.log("åˆ›å»ºæ•°æ®é›†ç¬¦å·é“¾æ¥...")
        
        dataset_dirs = ['dataset', 'dataset_classify']
        success_count = 0
        
        for dataset_dir in dataset_dirs:
            try:
                link_path = self.project_root / dataset_dir
                target_path = self.data_dir / dataset_dir
                
                # å¦‚æœç¬¦å·é“¾æ¥å·²å­˜åœ¨ï¼Œè·³è¿‡
                if link_path.exists() and link_path.is_symlink():
                    self.log(f"âš ï¸ ç¬¦å·é“¾æ¥å·²å­˜åœ¨: {dataset_dir}")
                    continue
                
                # å¦‚æœç›®æ ‡å­˜åœ¨ä¸”ä¸æ˜¯ç¬¦å·é“¾æ¥ï¼Œè·³è¿‡
                if link_path.exists() and not link_path.is_symlink():
                    self.log(f"âš ï¸ ç›®å½•å·²å­˜åœ¨ï¼ˆéç¬¦å·é“¾æ¥ï¼‰: {dataset_dir}")
                    continue
                
                # åˆ›å»ºç¬¦å·é“¾æ¥
                if target_path.exists():
                    link_path.symlink_to(target_path, target_is_directory=True)
                    self.log(f"âœ… åˆ›å»ºç¬¦å·é“¾æ¥: {dataset_dir} â†’ data/{dataset_dir}")
                    success_count += 1
                else:
                    self.log(f"âŒ ç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºç¬¦å·é“¾æ¥: data/{dataset_dir}")
            except Exception as e:
                self.log(f"âŒ åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥ {dataset_dir}: {str(e)}")
        
        return success_count
    
    def update_config_paths(self):
        """æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„"""
        self.log("æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„...")
        
        # æ›´æ–°ä¸»é…ç½®æ–‡ä»¶
        main_config_path = self.configs_dir / "config.json"
        if main_config_path.exists():
            try:
                import json
                with open(main_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # æ›´æ–°è·¯å¾„
                project_root_str = str(self.project_root)
                for key, value in config.items():
                    if isinstance(value, str) and project_root_str in value:
                        # å°†ç»å¯¹è·¯å¾„æ›´æ–°ä¸ºç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
                        new_value = value.replace(project_root_str, ".")
                        new_value = new_value.replace("/dataset/", "/data/dataset/")
                        config[key] = new_value
                
                # ä¿å­˜æ›´æ–°åçš„é…ç½®
                with open(main_config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                
                self.log("âœ… æ›´æ–°ä¸»é…ç½®æ–‡ä»¶è·¯å¾„")
            except Exception as e:
                self.log(f"âŒ æ›´æ–°ä¸»é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # æ›´æ–°åˆ†ç±»é…ç½®æ–‡ä»¶
        classify_config_path = self.configs_dir / "dataset_classify_config.json"
        if classify_config_path.exists():
            try:
                import json
                with open(classify_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # æ›´æ–°è·¯å¾„ä¸ºç›¸å¯¹è·¯å¾„
                if 'dataset_raw' in config:
                    config['dataset_raw'] = "./data/dataset_classify/dataset_raw"
                if 'dataset_processed' in config:
                    config['dataset_processed'] = "./data/dataset_classify/dataset_preprocess"
                
                # ä¿å­˜æ›´æ–°åçš„é…ç½®
                with open(classify_config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                
                self.log("âœ… æ›´æ–°åˆ†ç±»é…ç½®æ–‡ä»¶è·¯å¾„")
            except Exception as e:
                self.log(f"âŒ æ›´æ–°åˆ†ç±»é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def create_data_readme(self):
        """åœ¨dataç›®å½•åˆ›å»ºè¯´æ˜æ–‡ä»¶"""
        readme_content = f"""# Data Directory - æ•°æ®ç›®å½•

è¿™ä¸ªç›®å½•åŒ…å«äº†é¡¹ç›®çš„æ‰€æœ‰æ•°æ®é›†å’Œç›¸å…³æ–‡ä»¶ã€‚

## ç›®å½•ç»“æ„

### æ•°æ®é›†ç›®å½•
- `dataset/` - ä¸»è¦æ•°æ®é›†ç›®å½•
  - `dataset_raw/` - åŸå§‹æ•°æ®
  - `dataset_resized/` - è°ƒæ•´å¤§å°åçš„æ•°æ®
  - `dataset_preprocess/` - é¢„å¤„ç†åçš„æ•°æ®
  - `dataset_target/` - ç›®æ ‡æ•°æ®
- `dataset_classify/` - åˆ†ç±»æ•°æ®é›†ç›®å½•
  - `dataset_raw/` - åŸå§‹åˆ†ç±»æ•°æ®
  - `dataset_preprocess/` - é¢„å¤„ç†åçš„åˆ†ç±»æ•°æ®
  - `dataset_noise/` - å™ªå£°æ•°æ®
  - `dataset_preprocess2/` - äºŒæ¬¡é¢„å¤„ç†æ•°æ®

### æ•°æ®æ–‡ä»¶
- `dataset.zip` - æ•°æ®é›†å‹ç¼©åŒ…

## ç¬¦å·é“¾æ¥

ä¸ºäº†ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§ï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºäº†ä»¥ä¸‹ç¬¦å·é“¾æ¥ï¼š
- `dataset` â†’ `data/dataset`
- `dataset_classify` â†’ `data/dataset_classify`

## é…ç½®æ–‡ä»¶

ç›¸å…³çš„é…ç½®æ–‡ä»¶å·²ç§»åŠ¨åˆ° `configs/` ç›®å½•ï¼š
- `configs/config.json` - ä¸»é…ç½®æ–‡ä»¶
- `configs/dataset_classify_config.json` - åˆ†ç±»æ•°æ®é›†é…ç½®
- `configs/dataset_info.json` - æ•°æ®é›†ä¿¡æ¯

## æ•´ç†æ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ³¨æ„äº‹é¡¹
- æ•°æ®é›†ç›®å½•å·²ä»æ ¹ç›®å½•ç§»åŠ¨åˆ°æ­¤å¤„ä»¥ä¿æŒé¡¹ç›®ç»“æ„æ¸…æ™°
- é€šè¿‡ç¬¦å·é“¾æ¥ä¿æŒäº†å‘åå…¼å®¹æ€§
- é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„å·²ç›¸åº”æ›´æ–°
"""
        
        readme_path = self.data_dir / "README.md"
        readme_path.write_text(readme_content, encoding='utf-8')
        self.log(f"âœ… åˆ›å»ºæ•°æ®ç›®å½•è¯´æ˜æ–‡ä»¶: {readme_path}")
    
    def execute_supplement(self):
        """æ‰§è¡Œè¡¥å……æ•´ç†"""
        self.log("å¼€å§‹è¡¥å……æ•´ç†...")
        
        # 1. é¢„è§ˆæ“ä½œ
        existing_configs, existing_datasets, existing_others = self.preview_supplement()
        
        total_items = len(existing_configs) + len(existing_datasets) + len(existing_others)
        if total_items == 0:
            self.log("æ²¡æœ‰éœ€è¦è¡¥å……æ•´ç†çš„é¡¹ç›®")
            return True
        
        # 2. ç”¨æˆ·ç¡®è®¤
        print(f"\nğŸ“¦ å³å°†è¡¥å……æ•´ç† {total_items} ä¸ªé¡¹ç›®")
        print("è¿™ä¸ªæ“ä½œå°†:")
        print("- å°†é…ç½®æ–‡ä»¶å¤åˆ¶åˆ° configs/ ç›®å½•")
        print("- å°†æ•°æ®é›†ç›®å½•ç§»åŠ¨åˆ° data/ ç›®å½•")
        print("- åˆ›å»ºç¬¦å·é“¾æ¥ä¿æŒå…¼å®¹æ€§")
        print("- æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„")
        response = input("ç¡®è®¤æ‰§è¡Œè¡¥å……æ•´ç†ï¼Ÿ(y/N): ")
        
        if response.lower() != 'y':
            self.log("ç”¨æˆ·å–æ¶ˆè¡¥å……æ•´ç†æ“ä½œ")
            return False
        
        # 3. åˆ›å»ºç›®å½•ç»“æ„
        if not self.create_directories():
            self.log("âŒ åˆ›å»ºç›®å½•ç»“æ„å¤±è´¥")
            return False
        
        # 4. ç§»åŠ¨é…ç½®æ–‡ä»¶
        config_success = self.move_config_files(existing_configs)
        
        # 5. æ•´ç†æ•°æ®é›†
        dataset_success = self.organize_datasets(existing_datasets, existing_others)
        
        # 6. åˆ›å»ºç¬¦å·é“¾æ¥
        symlink_success = self.create_data_symlinks()
        
        # 7. æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„
        self.update_config_paths()
        
        # 8. åˆ›å»ºè¯´æ˜æ–‡ä»¶
        self.create_data_readme()
        
        # 9. æœ€ç»ˆéªŒè¯
        self.final_verification()
        
        total_success = config_success + dataset_success + symlink_success
        self.log(f"è¡¥å……æ•´ç†å®Œæˆ: æˆåŠŸå¤„ç† {total_success} ä¸ªé¡¹ç›®")
        
        return total_success > 0
    
    def final_verification(self):
        """æœ€ç»ˆéªŒè¯é¡¹ç›®ç»“æ„"""
        self.log("æ‰§è¡Œæœ€ç»ˆéªŒè¯...")
        
        print("\nğŸ“ è¡¥å……æ•´ç†åçš„é¡¹ç›®ç»“æ„:")
        print("=" * 60)
        
        # æ˜¾ç¤ºä¸»è¦ç›®å½•
        main_dirs = ['src', 'notebooks', 'scripts', 'tests', 'configs', 'docs', 'data', 'origin']
        for dir_name in main_dirs:
            dir_path = self.project_root / dir_name
            if dir_path.exists():
                print(f"âœ… {dir_name}/")
            else:
                print(f"âŒ {dir_name}/ (ç¼ºå¤±)")
        
        # æ˜¾ç¤ºconfigsç›®å½•å†…å®¹
        print(f"\nâš™ï¸ configs/ ç›®å½•å†…å®¹:")
        if self.configs_dir.exists():
            for item in self.configs_dir.iterdir():
                print(f"  ğŸ“„ {item.name}")
        
        # æ˜¾ç¤ºdataç›®å½•å†…å®¹
        print(f"\nğŸ’¾ data/ ç›®å½•å†…å®¹:")
        if self.data_dir.exists():
            for item in self.data_dir.iterdir():
                if item.is_dir():
                    print(f"  ğŸ“ {item.name}/")
                else:
                    print(f"  ğŸ“„ {item.name}")
        
        # æ˜¾ç¤ºç¬¦å·é“¾æ¥
        print(f"\nğŸ”— ç¬¦å·é“¾æ¥:")
        for link_name in ['dataset', 'dataset_classify']:
            link_path = self.project_root / link_name
            if link_path.exists() and link_path.is_symlink():
                target = link_path.readlink()
                print(f"  âœ… {link_name} â†’ {target}")
            elif link_path.exists():
                print(f"  ğŸ“ {link_name} (ç›®å½•ï¼Œéç¬¦å·é“¾æ¥)")
            else:
                print(f"  âŒ {link_name} (ä¸å­˜åœ¨)")
        
        self.log("æœ€ç»ˆéªŒè¯å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“¦ è¡¥å……æ•´ç†å·¥å…·")
    print("=" * 60)
    print("åŠŸèƒ½:")
    print("- å°†é—æ¼çš„é…ç½®æ–‡ä»¶æ•´ç†åˆ° configs/ ç›®å½•")
    print("- å°†æ•°æ®é›†ç›®å½•æ•´ç†åˆ° data/ ç›®å½•")
    print("- åˆ›å»ºç¬¦å·é“¾æ¥ä¿æŒå…¼å®¹æ€§")
    print("- æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„")
    print("- åˆ›å»ºè¯¦ç»†çš„è¯´æ˜æ–‡æ¡£")
    print("=" * 60)
    
    organizer = SupplementOrganizer()
    
    # æ‰§è¡Œè¡¥å……æ•´ç†
    if organizer.execute_supplement():
        print("\nâœ… è¡¥å……æ•´ç†æˆåŠŸå®Œæˆï¼")
        print("âš™ï¸ é…ç½®æ–‡ä»¶å·²æ•´ç†åˆ° configs/ ç›®å½•")
        print("ğŸ’¾ æ•°æ®é›†å·²æ•´ç†åˆ° data/ ç›®å½•")
        print("ğŸ”— åˆ›å»ºäº†ç¬¦å·é“¾æ¥ä¿æŒå…¼å®¹æ€§")
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {organizer.log_file}")
        print(f"ğŸ“– æŸ¥çœ‹è¯´æ˜: {organizer.data_dir}/README.md")
        return 0
    else:
        print("\nâŒ è¡¥å……æ•´ç†å¤±è´¥æˆ–è¢«å–æ¶ˆ")
        return 1

if __name__ == "__main__":
    sys.exit(main())